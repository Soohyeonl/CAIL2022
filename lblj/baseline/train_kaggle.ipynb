{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mktb/software/miniconda3/envs/cail/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict, Metric, load_dataset, load_metric\n",
    "from transformers import (\n",
    "    BatchEncoding,\n",
    "    EvalPrediction,\n",
    "    PreTrainedModel,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from transformers.tokenization_utils_base import (\n",
    "    PaddingStrategy,\n",
    "    PreTrainedTokenizerBase,\n",
    ")\n",
    "import evaluate\n",
    "\n",
    "\n",
    "def preprocess_function(\n",
    "    examples: Dict[str, List[str]], tokenizer: PreTrainedTokenizerBase\n",
    ") -> Dict[str, List[str]]:\n",
    "    sc_sentences: List[str] = sum([[f\"诉方称：{sc}\"] * 5 for sc in examples[\"sc\"]], [])\n",
    "\n",
    "    bc_sentences: List[str] = sum(\n",
    "        [\n",
    "            [f'辩方回应：{examples[f\"bc_{j}\"][i]}' for j in range(1, 6)]\n",
    "            for i in range(len(examples[\"id\"]))\n",
    "        ],\n",
    "        [],\n",
    "    )\n",
    "\n",
    "    tokenized_examples: BatchEncoding = tokenizer(\n",
    "        sc_sentences, bc_sentences, truncation=True, max_length=512\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        k: [v[i : i + 5] for i in range(0, len(v), 5)]\n",
    "        for k, v in tokenized_examples.items()\n",
    "    }\n",
    "\n",
    "\n",
    "def get_dataset(\n",
    "    location: str, tokenizer: PreTrainedTokenizerBase\n",
    ") -> Union[Dataset, DatasetDict]:\n",
    "    dataset: Dataset = load_dataset(\"json\", data_files=location, split=\"train\")\n",
    "\n",
    "    dataset = dataset.remove_columns(\n",
    "        ['text_id', 'category', 'chapter', 'crime']\n",
    "    ).map(\n",
    "        lambda x: preprocess_function(x, tokenizer), batched=True\n",
    "    )\n",
    "\n",
    "    if \"answer\" in dataset.column_names:\n",
    "        return dataset.rename_column(\"answer\", \"labels\").train_test_split(test_size=0.1)\n",
    "    else:\n",
    "        return dataset\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForMultipleChoice:\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
    "        batch_size = len(features)\n",
    "        num_choices = len(features[0][\"input_ids\"])\n",
    "\n",
    "        if \"labels\" in features[0].keys():\n",
    "            labels: Optional[List[int]] = [\n",
    "                feature.pop(\"labels\") for feature in features\n",
    "            ]\n",
    "        else:\n",
    "            labels = None\n",
    "\n",
    "        flattened_features: List[Dict[str, Any]] = sum(\n",
    "            [\n",
    "                [{k: v[i] for k, v in feature.items()} for i in range(num_choices)]\n",
    "                for feature in features\n",
    "            ],\n",
    "            [],\n",
    "        )\n",
    "\n",
    "        padded_features: BatchEncoding = self.tokenizer.pad(\n",
    "            flattened_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        batch: Dict[str, torch.Tensor] = {\n",
    "            k: v.view(batch_size, num_choices, -1) for k, v in padded_features.items()\n",
    "        }\n",
    "\n",
    "        if labels is not None:\n",
    "            batch[\"labels\"] = torch.tensor(labels, dtype=torch.int64) - 1\n",
    "        return batch\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred: EvalPrediction, metric: Metric) -> Dict[str, float]:\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "def get_trainer(\n",
    "    train_set: Optional[Dataset],\n",
    "    test_set: Optional[Dataset],\n",
    "    tokenizer: PreTrainedTokenizerBase,\n",
    "    model: PreTrainedModel,\n",
    ") -> Trainer:\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=5e-5,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=16,\n",
    "        gradient_accumulation_steps=4,\n",
    "        num_train_epochs=10,\n",
    "        weight_decay=0.01,\n",
    "        no_cuda=not torch.cuda.is_available(),\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_accuracy\",\n",
    "        greater_is_better=True,\n",
    "        optim=\"adamw_torch\",\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "\n",
    "    # metric = load_metric('accuracy')\n",
    "    metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "    return Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_set,\n",
    "        eval_dataset=test_set,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer),\n",
    "        compute_metrics=lambda x: compute_metrics(x, metric),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1694\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.ThreadTracer.__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '/home/mktb/software/miniconda3/envs/cail/lib/python3.10/site-packages/transformers/tokenization_utils_base.py'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/software/miniconda3/envs/cail/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd_file_utils.py:865\u001b[0m, in \u001b[0;36mget_abs_path_real_path_and_base_from_frame\u001b[0;34m(frame, NORM_PATHS_AND_BASE_CONTAINER)\u001b[0m\n\u001b[1;32m    864\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 865\u001b[0m     \u001b[39mreturn\u001b[39;00m NORM_PATHS_AND_BASE_CONTAINER[frame\u001b[39m.\u001b[39;49mf_code\u001b[39m.\u001b[39;49mco_filename]\n\u001b[1;32m    866\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m     \u001b[39m# This one is just internal (so, does not need any kind of client-server translation)\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: '/home/mktb/software/miniconda3/envs/cail/lib/python3.10/site-packages/transformers/tokenization_utils_base.py'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/software/miniconda3/envs/cail/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd_file_utils.py:829\u001b[0m, in \u001b[0;36mget_abs_path_real_path_and_base_from_file\u001b[0;34m(filename, NORM_PATHS_AND_BASE_CONTAINER)\u001b[0m\n\u001b[1;32m    828\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 829\u001b[0m     \u001b[39mreturn\u001b[39;00m NORM_PATHS_AND_BASE_CONTAINER[filename]\n\u001b[1;32m    830\u001b[0m \u001b[39mexcept\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: '/home/mktb/software/miniconda3/envs/cail/lib/python3.10/site-packages/transformers/tokenization_utils_base.py'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/software/miniconda3/envs/cail/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd_file_utils.py:390\u001b[0m, in \u001b[0;36m_abs_and_canonical_path\u001b[0;34m(filename, NORM_PATHS_CONTAINER)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 390\u001b[0m     \u001b[39mreturn\u001b[39;00m NORM_PATHS_CONTAINER[filename]\n\u001b[1;32m    391\u001b[0m \u001b[39mexcept\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: '/home/mktb/software/miniconda3/envs/cail/lib/python3.10/site-packages/transformers/tokenization_utils_base.py'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/mktb/Documents/github/Soohyeonl/CAIL2022/lblj/baseline/train_kaggle.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/mktb/Documents/github/Soohyeonl/CAIL2022/lblj/baseline/train_kaggle.ipynb#W1sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m set_seed(\u001b[39m42\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/mktb/Documents/github/Soohyeonl/CAIL2022/lblj/baseline/train_kaggle.ipynb#W1sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m model_card: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mbert-base-chinese\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/mktb/Documents/github/Soohyeonl/CAIL2022/lblj/baseline/train_kaggle.ipynb#W1sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m tokenizer: BertTokenizer \u001b[39m=\u001b[39m BertTokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(model_card)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mktb/Documents/github/Soohyeonl/CAIL2022/lblj/baseline/train_kaggle.ipynb#W1sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m model: BertForMultipleChoice \u001b[39m=\u001b[39m BertForMultipleChoice\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mktb/Documents/github/Soohyeonl/CAIL2022/lblj/baseline/train_kaggle.ipynb#W1sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     model_card\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mktb/Documents/github/Soohyeonl/CAIL2022/lblj/baseline/train_kaggle.ipynb#W1sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mktb/Documents/github/Soohyeonl/CAIL2022/lblj/baseline/train_kaggle.ipynb#W1sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m split_set: DatasetDict \u001b[39m=\u001b[39m get_dataset(\u001b[39m'\u001b[39m\u001b[39mdata/train_entry.jsonl\u001b[39m\u001b[39m'\u001b[39m, tokenizer)\n",
      "File \u001b[0;32m~/software/miniconda3/envs/cail/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1570\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1559\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1560\u001b[0m \u001b[39m    Returns the vocabulary as a dictionary of token to index.\u001b[39;00m\n\u001b[1;32m   1561\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1566\u001b[0m \u001b[39m        `Dict[str, int]`: The vocabulary.\u001b[39;00m\n\u001b[1;32m   1567\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1568\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m()\n\u001b[0;32m-> 1570\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m   1571\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_pretrained\u001b[39m(\u001b[39mcls\u001b[39m, pretrained_model_name_or_path: Union[\u001b[39mstr\u001b[39m, os\u001b[39m.\u001b[39mPathLike], \u001b[39m*\u001b[39minit_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   1572\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1573\u001b[0m \u001b[39m    Instantiate a [`~tokenization_utils_base.PreTrainedTokenizerBase`] (or a derived class) from a predefined\u001b[39;00m\n\u001b[1;32m   1574\u001b[0m \u001b[39m    tokenizer.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1646\u001b[0m \u001b[39m    assert tokenizer.unk_token == \"<unk>\"\u001b[39;00m\n\u001b[1;32m   1647\u001b[0m \u001b[39m    ```\"\"\"\u001b[39;00m\n\u001b[1;32m   1648\u001b[0m     cache_dir \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mcache_dir\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1363\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1696\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.ThreadTracer.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/software/miniconda3/envs/cail/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd_file_utils.py:883\u001b[0m, in \u001b[0;36mget_abs_path_real_path_and_base_from_frame\u001b[0;34m(frame, NORM_PATHS_AND_BASE_CONTAINER)\u001b[0m\n\u001b[1;32m    880\u001b[0m     i \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(f\u001b[39m.\u001b[39mrfind(\u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m), f\u001b[39m.\u001b[39mrfind(\u001b[39m'\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39m'\u001b[39m))\n\u001b[1;32m    881\u001b[0m     \u001b[39mreturn\u001b[39;00m f, f, f[i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m:]\n\u001b[0;32m--> 883\u001b[0m ret \u001b[39m=\u001b[39m get_abs_path_real_path_and_base_from_file(f)\n\u001b[1;32m    884\u001b[0m \u001b[39m# Also cache based on the frame.f_code.co_filename (if we had it inside build/bdist it can make a difference).\u001b[39;00m\n\u001b[1;32m    885\u001b[0m NORM_PATHS_AND_BASE_CONTAINER[frame\u001b[39m.\u001b[39mf_code\u001b[39m.\u001b[39mco_filename] \u001b[39m=\u001b[39m ret\n",
      "File \u001b[0;32m~/software/miniconda3/envs/cail/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd_file_utils.py:850\u001b[0m, in \u001b[0;36mget_abs_path_real_path_and_base_from_file\u001b[0;34m(filename, NORM_PATHS_AND_BASE_CONTAINER)\u001b[0m\n\u001b[1;32m    847\u001b[0m     \u001b[39melif\u001b[39;00m f\u001b[39m.\u001b[39mendswith(\u001b[39m'\u001b[39m\u001b[39m$py.class\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    848\u001b[0m         f \u001b[39m=\u001b[39m f[:\u001b[39m-\u001b[39m\u001b[39mlen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m$py.class\u001b[39m\u001b[39m'\u001b[39m)] \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.py\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 850\u001b[0m abs_path, canonical_normalized_filename \u001b[39m=\u001b[39m _abs_and_canonical_path(f)\n\u001b[1;32m    852\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    853\u001b[0m     base \u001b[39m=\u001b[39m os_path_basename(canonical_normalized_filename)\n",
      "File \u001b[0;32m~/software/miniconda3/envs/cail/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd_file_utils.py:413\u001b[0m, in \u001b[0;36m_abs_and_canonical_path\u001b[0;34m(filename, NORM_PATHS_CONTAINER)\u001b[0m\n\u001b[1;32m    410\u001b[0m abs_path \u001b[39m=\u001b[39m _apply_func_and_normalize_case(filename, os_path_abspath, isabs, normalize)\n\u001b[1;32m    412\u001b[0m normalize \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 413\u001b[0m real_path \u001b[39m=\u001b[39m _apply_func_and_normalize_case(filename, os_path_real_path, isabs, normalize)\n\u001b[1;32m    415\u001b[0m \u001b[39m# cache it for fast access later\u001b[39;00m\n\u001b[1;32m    416\u001b[0m NORM_PATHS_CONTAINER[filename] \u001b[39m=\u001b[39m abs_path, real_path\n",
      "File \u001b[0;32m~/software/miniconda3/envs/cail/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd_file_utils.py:440\u001b[0m, in \u001b[0;36m_apply_func_and_normalize_case\u001b[0;34m(filename, func, isabs, normalize_case, os_path_exists, join)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[39mif\u001b[39;00m filename\u001b[39m.\u001b[39mstartswith(\u001b[39m'\u001b[39m\u001b[39m<\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    436\u001b[0m     \u001b[39m# Not really a file, rather a synthetic name like <string> or <ipython-...>;\u001b[39;00m\n\u001b[1;32m    437\u001b[0m     \u001b[39m# shouldn't be normalized.\u001b[39;00m\n\u001b[1;32m    438\u001b[0m     \u001b[39mreturn\u001b[39;00m filename\n\u001b[0;32m--> 440\u001b[0m r \u001b[39m=\u001b[39m func(filename)\n\u001b[1;32m    442\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m isabs:\n\u001b[1;32m    443\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os_path_exists(r):\n",
      "File \u001b[0;32m~/software/miniconda3/envs/cail/lib/python3.10/posixpath.py:396\u001b[0m, in \u001b[0;36mrealpath\u001b[0;34m(filename, strict)\u001b[0m\n\u001b[1;32m    394\u001b[0m filename \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mfspath(filename)\n\u001b[1;32m    395\u001b[0m path, ok \u001b[39m=\u001b[39m _joinrealpath(filename[:\u001b[39m0\u001b[39m], filename, strict, {})\n\u001b[0;32m--> 396\u001b[0m \u001b[39mreturn\u001b[39;00m abspath(path)\n",
      "File \u001b[0;32m~/software/miniconda3/envs/cail/lib/python3.10/posixpath.py:385\u001b[0m, in \u001b[0;36mabspath\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    383\u001b[0m         cwd \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mgetcwd()\n\u001b[1;32m    384\u001b[0m     path \u001b[39m=\u001b[39m join(cwd, path)\n\u001b[0;32m--> 385\u001b[0m \u001b[39mreturn\u001b[39;00m normpath(path)\n",
      "File \u001b[0;32m~/software/miniconda3/envs/cail/lib/python3.10/posixpath.py:364\u001b[0m, in \u001b[0;36mnormpath\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[39mif\u001b[39;00m comp \u001b[39min\u001b[39;00m (empty, dot):\n\u001b[1;32m    363\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m--> 364\u001b[0m \u001b[39mif\u001b[39;00m (comp \u001b[39m!=\u001b[39m dotdot \u001b[39mor\u001b[39;00m (\u001b[39mnot\u001b[39;00m initial_slashes \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m new_comps) \u001b[39mor\u001b[39;00m\n\u001b[1;32m    365\u001b[0m      (new_comps \u001b[39mand\u001b[39;00m new_comps[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m dotdot)):\n\u001b[1;32m    366\u001b[0m     new_comps\u001b[39m.\u001b[39mappend(comp)\n\u001b[1;32m    367\u001b[0m \u001b[39melif\u001b[39;00m new_comps:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "from transformers import (BertForMultipleChoice, BertTokenizer, Trainer,\n",
    "                          set_seed)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    set_seed(42)\n",
    "    model_card: str = 'bert-base-chinese'\n",
    "\n",
    "    tokenizer: BertTokenizer = BertTokenizer.from_pretrained(model_card)\n",
    "    model: BertForMultipleChoice = BertForMultipleChoice.from_pretrained(\n",
    "        model_card\n",
    "    )\n",
    "\n",
    "    split_set: DatasetDict = get_dataset('data/train_entry.jsonl', tokenizer)\n",
    "    trainer: Trainer = get_trainer(split_set['train'], split_set['test'],\n",
    "                                   tokenizer, model)\n",
    "\n",
    "    trainer.train()\n",
    "    tokenizer.save_pretrained('model')\n",
    "    model.save_pretrained('model')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('cail')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "754629d8f7ff122d05a7f8a97b3b8fc9c66489009069b55e95a3c70f609b88b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
